# RobotArmScooper

![figure](./figure/figure.png)

RobotArmScooperëŠ” RobotArmSimulator ì— Unity MLAgent ë¥¼ ê²°í•©í•´ì„œ ì‹¤ì œë¡œ ë¡œë´‡íŒ”ë¡œ ê°•í™”í•™ìŠµì„ ì‹œë„í•´ë³¸ ìƒ˜í”Œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

ì´ˆë³´ì ì¸ ìˆ˜ì¤€ìœ¼ë¡œ Robot ìœ¼ë¡œ ìˆ˜ì €ë¥¼ ì§‘ëŠ” ê²ƒë§Œ í•´ë´¤ì§€ë§Œ, â€˜22ë…„ 4ì›” ì´í›„ ë” ì´ìƒ ì§„í–‰ì„ í•˜ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì— ì´ ì •ë„ì—ì„œ 1ì°¨ì ìœ¼ë¡œ public ê³µê°œí•©ë‹ˆë‹¤.

## ì‚¬ì „ ì„¤ì¹˜

ì´ í”„ë¡œì íŠ¸ë¥¼ ì œëŒ€ë¡œ ëŒë¦¬ê¸° ìœ„í•´ì„  ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì„ ì‚¬ì „ ì„¤ì¹˜í•´ì•¼í•©ë‹ˆë‹¤.

```
ğŸ“Œ 
C# ë° Unity ì§€ì› IDE (Visual Studio, Rider)
Unity 2021.3.X LTS version
ML-Agents Release 20
com.unity.ml-agents v2.3.0-exp.3
com.unity.ml-agents.extensions v0.6.1-preview
ml-agents v0.30.0
ml-agents-env v0.30.0
```

ì„¤ì¹˜ ë°©ë²•ì€ ì¸í„°ë„·ì„ ì°¸ê³ í•˜ë©´ ë©ë‹ˆë‹¤. ì°¸ê³ ë¡œ ì „ ì‚½ì§ˆ ëì— m1 macbook pro ì— ê°œë°œ í™˜ê²½ì„ ë§ˆë ¨í–ˆì—ˆìŠµë‹ˆë‹¤.

2023 ë…„ 1ì›” ê¸°ì¤€ìœ¼ë¡œ [ML-Agents Release 20](https://github.com/Unity-Technologies/ml-agents/releases/tag/release_20) ê¸°ì¤€ì—ì„œ ì´ìƒì—†ì´ ë™ì‘í•¨ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

## í›ˆë ¨ ë°©ë²•

![gif](./figure/train.gif)

git clone í›„ `RobotArmScooper` path ë¡œ ë“¤ì–´ê°„ ë‹¤ìŒ, ì•„ë˜ì™€ ê°™ì´ mlagents-learn ì„ ë™ì‘ì‹œí‚¨ë‹¤.

ë§Œì•½ conda ë¥¼ ì‚¬ìš©í•œë‹¤ë©´, ë¯¸ë¦¬ ml-agents ë¥¼ ì„¤ì¹˜í•´ë†“ì€ conda í™˜ê²½ì„ í™œì„±í™”ì‹œí‚¤ê³  ë™ì‘ì‹œì¼œì•¼í•œë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë™ì‘ ìì²´ê°€ ì•ˆëœë‹¤.

```bash
cd RobotArmScooper
mlagents-learn config/ppo/pickup_spoon.yaml --run-id=PickUpSpoon --force
```

ì»¤ë§¨ë“œë¥¼ ì…ë ¥í•˜ê³  Unity í™”ë©´ì—ì„œ play ë²„íŠ¼ì„ ëˆ„ë¥¸ë‹¤. ë²„íŠ¼ í´ë¦­ í›„ ì•„ë˜ì™€ ê°™ì´ Connect í™”ë©´ì´ íŒì—…ë˜ë©´ ì„±ê³µì´ë‹¤.

```bash

            â”  â•–
        â•“â•–â•¬â”‚â•¡  â”‚â”‚â•¬â•–â•–
    â•“â•–â•¬â”‚â”‚â”‚â”‚â”‚â”˜  â•¬â”‚â”‚â”‚â”‚â”‚â•¬â•–
 â•–â•¬â”‚â”‚â”‚â”‚â”‚â•¬â•œ        â•™â•¬â”‚â”‚â”‚â”‚â”‚â•–â•–                               â•—â•—â•—
 â•¬â•¬â•¬â•¬â•–â”‚â”‚â•¦â•–        â•–â•¬â”‚â”‚â•—â•£â•£â•£â•¬      â•Ÿâ•£â•£â•¬    â•Ÿâ•£â•£â•£             â•œâ•œâ•œ  â•Ÿâ•£â•£
 â•¬â•¬â•¬â•¬â•¬â•¬â•¬â•¬â•–â”‚â•¬â•–â•–â•“â•¬â•ªâ”‚â•“â•£â•£â•£â•£â•£â•£â•£â•¬      â•Ÿâ•£â•£â•¬    â•Ÿâ•£â•£â•£ â•’â•£â•£â•–â•—â•£â•£â•£â•—   â•£â•£â•£ â•£â•£â•£â•£â•£â•£ â•Ÿâ•£â•£â•–   â•£â•£â•£
 â•¬â•¬â•¬â•¬â”  â•™â•¬â•¬â•¬â•¬â”‚â•“â•£â•£â•£â•â•œ  â•«â•£â•£â•£â•¬      â•Ÿâ•£â•£â•¬    â•Ÿâ•£â•£â•£ â•Ÿâ•£â•£â•£â•™ â•™â•£â•£â•£  â•£â•£â•£ â•™â•Ÿâ•£â•£â•œâ•™  â•«â•£â•£  â•Ÿâ•£â•£
 â•¬â•¬â•¬â•¬â”     â•™â•¬â•¬â•£â•£      â•«â•£â•£â•£â•¬      â•Ÿâ•£â•£â•¬    â•Ÿâ•£â•£â•£ â•Ÿâ•£â•£â•¬   â•£â•£â•£  â•£â•£â•£  â•Ÿâ•£â•£     â•£â•£â•£â”Œâ•£â•£â•œ
 â•¬â•¬â•¬â•œ       â•¬â•¬â•£â•£      â•™â•â•£â•£â•¬      â•™â•£â•£â•£â•—â•–â•“â•—â•£â•£â•£â•œ â•Ÿâ•£â•£â•¬   â•£â•£â•£  â•£â•£â•£  â•Ÿâ•£â•£â•¦â•“    â•£â•£â•£â•£â•£
 â•™   â•“â•¦â•–    â•¬â•¬â•£â•£   â•“â•—â•—â•–            â•™â•â•£â•£â•£â•£â•â•œ   â•˜â•â•â•œ   â•â•â•  â•â•â•   â•™â•£â•£â•£    â•Ÿâ•£â•£â•£
   â•©â•¬â•¬â•¬â•¬â•¬â•¬â•¦â•¦â•¬â•¬â•£â•£â•—â•£â•£â•£â•£â•£â•£â•£â•                                             â•«â•£â•£â•£â•£
      â•™â•¬â•¬â•¬â•¬â•¬â•¬â•¬â•£â•£â•£â•£â•£â•£â•â•œ
          â•™â•¬â•¬â•¬â•£â•£â•£â•œ
             â•™
        
 Version information:
  ml-agents: 0.30.0,
  ml-agents-envs: 0.30.0,
  Communicator API: 1.5.0,
  PyTorch: 1.8.0.post3
[INFO] Listening on port 5004. Start training by pressing the Play button in the Unity Editor.
[INFO] Connected to Unity environment with package version 2.3.0-exp.3 and communication version 1.5.0
[INFO] Connected new brain: PickUpSpoon?team=0
[WARNING] Deleting TensorBoard data events.out.tfevents.1647788529.yuncheoljung-ui-MacBookPro.local.6858.0 that was left over from a previous run.
[INFO] Hyperparameters for behavior name PickUpSpoon: 
        trainer_type:   ppo
        hyperparameters:        
          batch_size:   64
          buffer_size:  12000
          learning_rate:        0.0003
          beta: 0.001
          epsilon:      0.2
          lambd:        0.99
          num_epoch:    3
          shared_critic:        False
          learning_rate_schedule:       linear
          beta_schedule:        linear
          epsilon_schedule:     linear
        network_settings:       
          normalize:    True
          hidden_units: 128
          num_layers:   2
          vis_encode_type:      simple
          memory:       None
          goal_conditioning_type:       hyper
          deterministic:        False
        reward_signals: 
          extrinsic:    
            gamma:      0.99
            strength:   1.0
            network_settings:   
              normalize:        False
              hidden_units:     128
              num_layers:       2
              vis_encode_type:  simple
              memory:   None
              goal_conditioning_type:   hyper
              deterministic:    False
        init_path:      None
        keep_checkpoints:       5
        checkpoint_interval:    500000
        max_steps:      500000
        time_horizon:   1000
        summary_freq:   12000
        threaded:       False
        self_play:      None
        behavioral_cloning:     None
```

ì •ìƒì ìœ¼ë¡œ í›ˆë ¨ì´ ì§„í–‰ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ Log ê°€ ê³„ì† ì¶œë ¥ëœë‹¤.

```bash
[INFO] PickUpSpoon. Step: 12000. Time Elapsed: 46.634 s. Mean Reward: -43.007. Std of Reward: 75.669. Training.
[INFO] PickUpSpoon. Step: 24000. Time Elapsed: 84.588 s. Mean Reward: -38.279. Std of Reward: 84.488. Training.
[INFO] PickUpSpoon. Step: 36000. Time Elapsed: 133.120 s. Mean Reward: -35.499. Std of Reward: 53.721. Training.
[INFO] PickUpSpoon. Step: 48000. Time Elapsed: 171.077 s. Mean Reward: -15.993. Std of Reward: 74.659. Training.
[INFO] PickUpSpoon. Step: 60000. Time Elapsed: 211.006 s. Mean Reward: -16.596. Std of Reward: 57.959. Training.
[INFO] PickUpSpoon. Step: 72000. Time Elapsed: 250.905 s. Mean Reward: -11.014. Std of Reward: 51.957. Training.
```

## Test ë°©ë²•

![gif](./figure/eval.gif)

í›ˆë ¨ ì™„ë£Œ í›„, ë³„ë„ì˜ ì„¤ì • ì—†ì´ Unity ì—ì„œ play ë¥¼ ëˆ„ë¥´ë©´ ë™ì‘í•œë‹¤.